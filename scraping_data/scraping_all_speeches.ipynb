{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "#### The following code is to **rescrape all the speeches from the UCSB website**\n",
    "#### To access the data, you can find it in \"all_presidential_speeches.csv\"\n",
    "\n",
    "## You will likely NOT need to run this entire file unless you want to rescrape**\n",
    "#### The following code takes 25-30 minutes to run everything, so it's best to access the pre-scraped data in \"all_presidential_speeches.csv\"\n",
    "\n",
    "- It will return a pandas dataframe (df) of 6000+ speeches in the categories found in \"list_to_scrape.csv\"\n",
    "- To edit the categories, refer to list_to_scrape.csv\n",
    "- The scraped data is in a csv called \"all_presidential_speeches.csv\"\n",
    "\n",
    "\n",
    "# To run everything:\n",
    "- uncomment scrape_all( ) to run everthing\n",
    "\n",
    "# How it works:\n",
    "- scrape_all( ) calls the following:\n",
    "    - list_to_scrape.csv\n",
    "    - dict_of_all_presidents( )\n",
    "    - feed_urls ( ) which calls the following:\n",
    "        - create_pages( )\n",
    "        - get_links( )\n",
    "        - get_speech( )\n",
    "    - returns all speeches from all categories (i.e. oral address, farewell addresses, etc.) into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url):\n",
    "    content = requests.get(url)\n",
    "    return BeautifulSoup(content.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict_of_all_presidents( ):\n",
    "- returns a dict of presidents mapped to an id_number\n",
    "- is called by scrape_all( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_of_all_presidents():\n",
    "    all_presidents_url = 'https://www.presidency.ucsb.edu/presidents'\n",
    "    soup = scrape(all_presidents_url)\n",
    "    soup.prettify().encode('utf8')\n",
    "    texts = soup.findAll(text=True)\n",
    "    start_index = texts.index('Donald J. Trump')\n",
    "    end_index = texts.index('George Washington')\n",
    "    texts = texts[start_index: end_index+1]\n",
    "    dict_all_prezs = dict()\n",
    "    count = 45\n",
    "    for elem in texts:\n",
    "        if not elem.isdigit() and elem != ' to ' and elem != ' ' and elem != '\\n':\n",
    "            dict_all_prezs[elem] = count\n",
    "            count -= 1\n",
    "    # print(dict_all_prezs)\n",
    "    return dict_all_prezs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters: url, page_count\n",
    "- page_count is how many items to show per page\n",
    "    - page count comes from list_to_scrape of how many items there are per speech\n",
    "\n",
    "- create_pages( ):\n",
    "    - is called by feed_urls( )\n",
    "    - returns an array [ ] with all the links for that category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pages(url, page_count):\n",
    "    items_per_page = 'items_per_page=10'\n",
    "    page_lst = []\n",
    "    page_lst.append(url + \"?\" + items_per_page)\n",
    "    for i in range(1,(page_count-1)//10):\n",
    "        page = \"page=\" + str(i)\n",
    "        page_lst.append(url + \"?\" + page + \"&\" + items_per_page)\n",
    "    return page_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get_speech( ) \n",
    "    - is called by feed_urls( )\n",
    "    - scrapes the text of the speech and places it into a df \n",
    "    - returns a df with the following categories:\n",
    "        - title\n",
    "        - date\n",
    "        - year\n",
    "        - president\n",
    "        - president_id\n",
    "        - content\n",
    "        - link2site\n",
    "        - footnote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speech(url, dict_all_prezs): \n",
    "    soup = scrape(url)\n",
    "    soup.prettify().encode('utf8')\n",
    "    #title\n",
    "    if soup.find(\"div\", {\"class\": \"field-ds-doc-title\"}) == None:\n",
    "        title = \"None\"\n",
    "    else:\n",
    "        title = soup.find(\"div\", {\"class\": \"field-ds-doc-title\"}).get_text().strip() \n",
    "    #content\n",
    "    if soup.find(\"div\", {\"class\": \"field-docs-content\"}) == None:\n",
    "        content = \"None\"\n",
    "    else:\n",
    "        content = soup.find(\"div\", {\"class\": \"field-docs-content\"}).get_text().strip()\n",
    "    #date\n",
    "    if soup.find(\"span\", {\"class\": \"date-display-single\"}) == None:\n",
    "        date = \"None\"\n",
    "    else:\n",
    "        date = soup.find(\"span\", {\"class\": \"date-display-single\"}).get_text().strip()\n",
    "    #president\n",
    "    if soup.find(\"div\", {\"class\": \"field-title\"}) == None:\n",
    "        president = \"None\"\n",
    "    else: \n",
    "        president = soup.find(\"div\", {\"class\": \"field-title\"}).get_text().strip()\n",
    "    #removes non president speeches\n",
    "    if president not in dict_all_prezs: \n",
    "        return None\n",
    "    president_id = dict_all_prezs[president]\n",
    "    #url to the direct website \n",
    "    if soup.find(\"div\", {\"class\": \"field-prez-document-citation\"}) == None:\n",
    "        link2site = \"None\"\n",
    "    else:\n",
    "        link2site = soup.find(\"div\", {\"class\": \"field-prez-document-citation\"}).get_text().strip()\n",
    "        link2site_index = link2site.find('https')\n",
    "        link2site = link2site[link2site_index:]\n",
    "    #footnotes\n",
    "    if soup.find(\"div\", {\"class\": \"field-docs-footnote\"}) == None:\n",
    "        footnote = \"None\"\n",
    "    else:\n",
    "        footnote = soup.find(\"div\", {\"class\": \"field-docs-footnote\"}).get_text().strip()\n",
    "    year = date.strip()[:-5:-1][::-1]\n",
    "    df = pd.DataFrame({'title': [title],\n",
    "                   'date': [date],\n",
    "                   'year': [year],\n",
    "                   'president': [president],\n",
    "                   'president_id': [president_id],\n",
    "                   'content': [content],\n",
    "                   'url': [link2site],\n",
    "                   'footnote': [footnote]\n",
    "                   })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- feed_urls( ): \n",
    "    - is called by scrape_all( )\n",
    "    - calls create_pages( ), get_links( ), get_speech( )\n",
    "    - returns a df for getting the speeches for that category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_urls(count, name, url, dict_all_prezs):\n",
    "    print('Starting scrape for ' + name)\n",
    "    pages = create_pages(url, count)\n",
    "    all_speeches = []\n",
    "    count = 1\n",
    "    for each_page in pages:\n",
    "        for i in get_links(each_page):\n",
    "            spch = get_speech(i, dict_all_prezs)\n",
    "            if spch != None:\n",
    "                print('NOT APPENDED', spch['title'])\n",
    "                all_speeches.append(spch)\n",
    "    col_names = ['title', 'date', 'year', 'president', 'president_id', 'content', 'url', 'footnote']\n",
    "    df = pd.DataFrame(all_speeches, columns=col_names)\n",
    "    print('Finished scraping' + name, '\\n', df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get_links(): \n",
    "    - is called by feed_urls()\n",
    "    - opens each individual speech from that category\n",
    "    - returns all_links from that category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(url): #\n",
    "    soup = scrape(url)\n",
    "    soup.prettify().encode('utf8')\n",
    "    content = soup.find_all(\"div\", {\"class\": \"field-title\"})\n",
    "    all_links = []\n",
    "    for i in content:\n",
    "        link = \"https://www.presidency.ucsb.edu/\" + str(i.find(\"a\").get(\"href\"))\n",
    "        all_links.append(link)\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- removeQ_A( ):\n",
    "    - takes in the df that contains all the speeches from every category\n",
    "    - drops any speech that is a Q and A speech\n",
    "    - returns df with removed Q and A speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeQ_A(df):\n",
    "    dropLst = []\n",
    "    for index, row in df.iterrows():\n",
    "        dropLst.append(index)\n",
    "        print(index)\n",
    "        break\n",
    "        try:\n",
    "            if(len(row['content'])) < 10 or row['content'].find('Q. ') != -1:\n",
    "                dropLst.append(index)\n",
    "        except TypeError:\n",
    "            dropLst.append(index)\n",
    "    dropLst.pop()\n",
    "    dropLst.pop()\n",
    "    for i in dropLst:\n",
    "        df = df.drop(index=i)\n",
    "    keep_col =  ['title', 'date', 'year', 'president', 'president_id', 'content', 'url', 'footnote', 'speech_type']\n",
    "    df = df[keep_col]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scrape_all( ):\n",
    "    - calls dict_of_all_presidents( )\n",
    "    - calls feed_urls that scrape all the cateogries from \"list_to_scrape.csv\"\n",
    "        - feed_urls( ) calls create_pages( ), get_links( ), get_speech( )\n",
    "\n",
    "TO DO:\n",
    "- Go to the UCSB website to look at which categories of speeches you want to scrape\n",
    "- add those categories to list_to_scrape.csv\n",
    "- _IF YOU WANT TO RERUN EVERYTHING_, uncomment \"scrape_all()\" in the following code chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all():\n",
    "    # TO DO: Change the csv_file to your csv_file\n",
    "    csv_file = '/Users/Andey/Desktop/presidential_speeches/scraping_data/list_to_scrape.csv'\n",
    "    dict_all_prezs = dict_of_all_presidents()\n",
    "    df_scrape_categories = pd.read_csv(csv_file)\n",
    "    frames = []\n",
    "    for i in range(len(df_scrape_categories)):\n",
    "        count = df.iloc[i]['count']\n",
    "        name = df.iloc[i]['name']\n",
    "        url = df.iloc[i]['url']\n",
    "        \n",
    "        #df_speech: all speeches of \"name\" category (i.e. oral_address)\n",
    "        df_speech = feed_urls(count, name, url, dict_all_prezs)\n",
    "        df_speech['speech_type'] = name\n",
    "        frames.append(df_speech)\n",
    "        \n",
    "    #concatenates all speech types (i.e. oral address, farewell speeches, etc.) into 1 df\n",
    "    result = pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "    #drops duplicates\n",
    "    result.drop_duplicates(subset=['content']) \n",
    "    \n",
    "    #TO DO: Change the path to your output path\n",
    "    all_prez_df_path = '/Users/Andey/Desktop/presidential_speeches/scraping_data/all_presidential_speeches.csv'\n",
    "    keep_col =  ['title', 'date', 'year', 'president', 'president_id', 'content', 'url', 'footnote', 'speech_type']\n",
    "    new_df = result[keep_col]\n",
    "    removeQ_A(new_df) \n",
    "    \n",
    "    #writing new_df into a csv with the path above\n",
    "    new_df.to_csv(all_prez_df_path, index=True)\n",
    "        \n",
    "    return None\n",
    "\n",
    "#TO DO: uncomment scrape_all() to run everthing\n",
    "#scrape_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
